{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7fae87af-4c1e-45cd-b792-9f2151519515",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/datasets/Sp1786/multiclass-sentiment-analysis-dataset \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "torch.manual_seed(42) \n",
    "from transformers import logging\n",
    "logging.set_verbosity_error()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "76f58258-b00b-433d-885f-d1347904ca77",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.head of           id                                               text  label  \\\n",
       "0       9536                    Cooking microwave pizzas, yummy      2   \n",
       "1       6135  Any plans of allowing sub tasks to show up in ...      1   \n",
       "2      17697   I love the humor, I just reworded it. Like sa...      2   \n",
       "3      14182                       naw idk what ur talkin about      1   \n",
       "4      17840          That sucks to hear. I hate days like that      0   \n",
       "...      ...                                                ...    ...   \n",
       "31227   6265   Grrrr....I got the wrong size coat for the sheep      0   \n",
       "31228  11284                              4 cases of swine flu!      1   \n",
       "31229   6436                                          excellent      2   \n",
       "31230    860  is sitting thru the boring bits in Titanic wai...      1   \n",
       "31231  15795                                    Missed the play      0   \n",
       "\n",
       "      sentiment  \n",
       "0      positive  \n",
       "1       neutral  \n",
       "2      positive  \n",
       "3       neutral  \n",
       "4      negative  \n",
       "...         ...  \n",
       "31227  negative  \n",
       "31228   neutral  \n",
       "31229  positive  \n",
       "31230   neutral  \n",
       "31231  negative  \n",
       "\n",
       "[31232 rows x 4 columns]>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "splits = {'train': 'train_df.csv', 'validation': 'val_df.csv', 'test': 'test_df.csv'}\n",
    "df = pd.read_csv(\"hf://datasets/Sp1786/multiclass-sentiment-analysis-dataset/\" + splits[\"train\"])\n",
    "df_test = pd.read_csv(\"hf://datasets/Sp1786/multiclass-sentiment-analysis-dataset/\" + splits[\"test\"])\n",
    "df.head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2cad2c7c-9f9a-48ad-b76c-ff4d46eb17af",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "label\n",
       "1    11649\n",
       "2    10478\n",
       "0     9105\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.label.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "ac58fa84-3d42-4f4a-a620-df9b0abb32e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of labels: 3\n"
     ]
    }
   ],
   "source": [
    "print('Number of labels:', df.label.nunique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "85c0f656-34f5-4ac1-968f-c357a853f0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_texts = df['text'].tolist()\n",
    "train_labels = df['label'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0c9a5b28-b453-44f0-960c-98d3640a3447",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['cooking', 'microwave', 'pizzas', 'yummy']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import re\n",
    "def tokenize(text):\n",
    "    tokenized_review_text = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    return tokenized_review_text\n",
    "\n",
    "tokenized_corpus = [tokenize(review_text) for review_text in train_texts]\n",
    "tokenized_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "4a03e505-9e88-4b65-8dc3-2126ab1b2182",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "combined_corpus = []\n",
    "for text in tokenized_corpus:\n",
    "    for token in text:\n",
    "        combined_corpus.append(token)\n",
    "\n",
    "word_freqs = Counter(combined_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "32505226-df44-4e7e-9187-8695a00e0d57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 Most Common Words:  [('i', 24290), ('to', 18191), ('the', 18108), ('it', 12243), ('a', 11488), ('and', 10254), ('my', 7651), ('is', 7306), ('you', 7123), ('for', 6898)]\n"
     ]
    }
   ],
   "source": [
    "MAX_VOCAB_SIZE = 1000\n",
    "most_common_words = word_freqs.most_common(MAX_VOCAB_SIZE)\n",
    "print(\"Top 10 Most Common Words: \", most_common_words[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "9f85f60f-6ca8-4fd9-9905-3248fc16e802",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = {word: idx + 2 for idx, (word, freq) in enumerate(most_common_words)}\n",
    "vocab['<unk>'] = 0\n",
    "vocab['<pad>'] = 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "4a2d7433-b615-4a57-be15-d93c3b9a54cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode_text(text, vocab):\n",
    "    tokenized_text = tokenize(text)\n",
    "    encoded_text = [vocab.get(word, vocab['<unk>']) for word in tokenized_text]\n",
    "    return encoded_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "56c84ac4-31a7-4133-9c59-7322047b4812",
   "metadata": {},
   "outputs": [],
   "source": [
    "def pad_or_truncate(encoded_text, max_len):\n",
    "    if len(encoded_text) > max_len:\n",
    "        return encoded_text[:max_len]\n",
    "    else:\n",
    "        return encoded_text + [vocab['<pad>']] * (max_len - len(encoded_text)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "3d513fde-df96-4f7a-85d5-bc67bb64ed07",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SEQ_LENGTH = 128\n",
    "padded_text_seqs = [pad_or_truncate(encode_text(text, vocab), max_len=MAX_SEQ_LENGTH) for text in train_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "142a867a-4fce-4fc1-9e8e-2f357918db71",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "X_tensor = torch.tensor(padded_text_seqs)\n",
    "y_tensor = torch.tensor(train_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "batch_size = 16\n",
    "train_dataset = TensorDataset(X_tensor, y_tensor)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "496e2565-4f59-410c-a30a-8ce263d1de07",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "torch.manual_seed(42) \n",
    "\n",
    "class SimpleNNWithEmbedding(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size):\n",
    "        super(SimpleNNWithEmbedding, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.fc1 = nn.Linear(embed_size, hidden_size)\n",
    "        self.fc2 = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        x = torch.mean(x, dim=1)\n",
    "        x = self.fc1(x)\n",
    "        x = torch.relu(x)\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "c62de7b1-de43-4f9c-94d1-c2351da28845",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SimpleNNWithEmbedding(\n",
      "  (embedding): Embedding(1002, 50)\n",
      "  (fc1): Linear(in_features=50, out_features=100, bias=True)\n",
      "  (fc2): Linear(in_features=100, out_features=3, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "vocab_size = len(vocab)\n",
    "embed_size = 50 \n",
    "hidden_size = 100\n",
    "output_size = 3\n",
    "\n",
    "text_classifier_nn = SimpleNNWithEmbedding(vocab_size, embed_size, hidden_size, output_size)\n",
    "print(text_classifier_nn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "8fb47aa5-f60c-4bd7-9f81-ca16a5d2fbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(text_classifier_nn.parameters(), lr=0.005)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "bbec6d1f-4109-4b40-98d1-4d77b944dcf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 5/50], Average CE Loss: 0.7451\n",
      "[Epoch 10/50], Average CE Loss: 0.6842\n",
      "[Epoch 15/50], Average CE Loss: 0.6340\n",
      "[Epoch 20/50], Average CE Loss: 0.5839\n",
      "[Epoch 25/50], Average CE Loss: 0.5389\n",
      "[Epoch 30/50], Average CE Loss: 0.5020\n",
      "[Epoch 35/50], Average CE Loss: 0.4679\n",
      "[Epoch 40/50], Average CE Loss: 0.4371\n",
      "[Epoch 45/50], Average CE Loss: 0.4192\n",
      "[Epoch 50/50], Average CE Loss: 0.3960\n"
     ]
    }
   ],
   "source": [
    "def train_model(model, train_loader, criterion, optimizer, num_epochs=5):\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        epoch_loss = 0.0\n",
    "        for batch_X, batch_y in train_loader:\n",
    "            optimizer.zero_grad()\n",
    "            output = model(batch_X)\n",
    "            loss = criterion(output, batch_y)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item()\n",
    "\n",
    "        avg_loss = epoch_loss / len(train_loader)\n",
    "        if (epoch + 1) % 5 == 0:\n",
    "            print(f\"[Epoch {epoch + 1}/{num_epochs}], Average CE Loss: {avg_loss:.4f}\")\n",
    "\n",
    "train_model(text_classifier_nn, train_dataloader, criterion, optimizer, num_epochs=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ac55682f-6139-446a-92b9-390f8a9fae08",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_texts = df['text'].tolist()\n",
    "test_labels = df['label'].tolist()\n",
    "padded_text_seqs_test = [pad_or_truncate(encode_text(test_seq, vocab), MAX_SEQ_LENGTH) for test_seq in test_texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "e30b5c0f-d7c1-4b8c-905b-af791d604564",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_tensor_test = torch.tensor(padded_text_seqs_test)\n",
    "y_tensor_test = torch.tensor(test_labels, dtype=torch.long)\n",
    "\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "test_dataset = TensorDataset(X_tensor_test, y_tensor_test)\n",
    "test_loader = DataLoader(test_dataset, batch_size=8, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "ccfd32fb-9b61-4425-a190-e0d40a54c8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def get_predictions_and_probabilities(model, test_loader):\n",
    "    model.eval()\n",
    "\n",
    "    all_probs = [] \n",
    "    all_labels = []\n",
    "\n",
    "    with torch.no_grad(): \n",
    "        for batch_X, batch_y in test_loader:\n",
    "            outputs = model(batch_X)\n",
    "            probs = F.softmax(outputs, dim=1)  \n",
    "            all_probs.extend(probs.cpu().numpy())\n",
    "            predicted_labels = torch.argmax(outputs, dim=1)\n",
    "            all_labels.extend(predicted_labels.cpu().numpy())\n",
    "\n",
    "    return all_probs, all_labels\n",
    "\n",
    "pred_probs, pred_labels = get_predictions_and_probabilities(text_classifier_nn, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "f5253b08-95da-4e56-8d5b-aae3cf1fcf56",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7379 1264  462]\n",
      " [ 955 9637 1057]\n",
      " [ 235  922 9321]]\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.86      0.81      0.84      9105\n",
      "           1       0.82      0.83      0.82     11649\n",
      "           2       0.86      0.89      0.87     10478\n",
      "\n",
      "    accuracy                           0.84     31232\n",
      "   macro avg       0.85      0.84      0.84     31232\n",
      "weighted avg       0.84      0.84      0.84     31232\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "conf_matrix = confusion_matrix(test_labels, pred_labels)\n",
    "report = classification_report(test_labels, pred_labels)\n",
    "\n",
    "print(conf_matrix)\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f7f1985-ec63-4b8c-a305-05a25633f571",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
